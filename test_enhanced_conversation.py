#!/usr/bin/env python3
"""
Enhanced Conversation Quality Test
Test the improved prompting system with focus on:
- Conversation continuity (target: 80+)
- User engagement (target: 80+)  
- Care and respect (target: 85+)
"""

import requests
import json
import time
import random
from typing import Dict, List

class EnhancedConversationTester:
    def __init__(self):
        self.api_url = "https://wisbee-router.yukihamada.workers.dev/v1/chat/completions"
        self.test_results = []
        self.conversation_scenarios = [
            {
                "category": "æŠ€è¡“è§£èª¬",
                "initial_message": "Pythonã®ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã«ã¤ã„ã¦æ•™ãˆã¦",
                "follow_up_context": "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…ã§ã€ã¾ã é–¢æ•°ã‚‚å®Œå…¨ã«ç†è§£ã§ãã¦ã„ãªã„",
                "expected_elements": ["è³ªå•ã§çµ‚ã‚ã‚‹", "ç†è§£åº¦ã¸ã®é…æ…®", "å…·ä½“ä¾‹", "æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ææ¡ˆ"]
            },
            {
                "category": "å­¦ç¿’æ”¯æ´", 
                "initial_message": "è‹±èªã®å‹‰å¼·ãŒå…¨ç„¶é€²ã¾ãªãã¦å›°ã£ã¦ã¾ã™",
                "follow_up_context": "TOEICã®ã‚¹ã‚³ã‚¢ã‚¢ãƒƒãƒ—ãŒç›®æ¨™ã§ã€ç‰¹ã«ãƒªã‚¹ãƒ‹ãƒ³ã‚°ãŒè‹¦æ‰‹",
                "expected_elements": ["å…±æ„Ÿ", "åŠ±ã¾ã—", "å…·ä½“çš„å­¦ç¿’è¨ˆç”»", "ç¶™ç¶šã¸ã®è³ªå•"]
            },
            {
                "category": "é›‘è«‡",
                "initial_message": "ä»Šæ—¥ã¯é›¨ã§æ°—åˆ†ãŒæ²ˆã‚“ã§ã¾ã™",
                "follow_up_context": "åœ¨å®…å‹¤å‹™ã§ä¸€äººã§ä½œæ¥­ã—ã¦ã„ã¦ã€å°‘ã—å­¤ç‹¬æ„Ÿã‚’æ„Ÿã˜ã¦ã„ã‚‹",
                "expected_elements": ["æ°—æŒã¡ã¸ã®å…±æ„Ÿ", "è©±é¡Œå±•é–‹", "ç›¸æ‰‹ã¸ã®é–¢å¿ƒ", "æ¸©ã‹ã„é›°å›²æ°—"]
            },
            {
                "category": "æ‚©ã¿ç›¸è«‡",
                "initial_message": "è·å ´ã®äººé–“é–¢ä¿‚ãŒã†ã¾ãã„ã‹ãšã‚¹ãƒˆãƒ¬ã‚¹ãŒæºœã¾ã£ã¦ã¾ã™",
                "follow_up_context": "ä¸Šå¸ã¨ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ç‰¹ã«æ‚©ã‚“ã§ã„ã‚‹",
                "expected_elements": ["æ·±ã„å…±æ„Ÿ", "å®‰å…¨ãªç’°å¢ƒ", "ç¶™ç¶šã‚µãƒãƒ¼ãƒˆ", "å…·ä½“çš„åŠ©è¨€"]
            },
            {
                "category": "å‰µä½œæ”¯æ´",
                "initial_message": "å°èª¬ã‚’æ›¸ã„ã¦ã„ã‚‹ã®ã§ã™ãŒã€ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãŒè–„ã£ãºã‚‰ããªã£ã¦ã—ã¾ã„ã¾ã™",
                "follow_up_context": "ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼å°èª¬ã‚’æ›¸ã„ã¦ã„ã¦ã€ä¸»äººå…¬ã®æˆé•·ç‰©èª",
                "expected_elements": ["å‰µä½œæ„æ¬²ã®å°Šé‡", "å»ºè¨­çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯", "å‰µä½œãƒ—ãƒ­ã‚»ã‚¹ã¸ã®é–¢å¿ƒ", "æŠ€è¡“çš„ã‚µãƒãƒ¼ãƒˆ"]
            }
        ]

    def make_api_request(self, message: str) -> Dict:
        """API ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡"""
        payload = {
            "model": "wisbee-router",
            "messages": [{"role": "user", "content": message}],
            "max_tokens": 500,
            "temperature": 0.7
        }
        
        try:
            response = requests.post(self.api_url, json=payload, timeout=30)
            return response.json()
        except Exception as e:
            return {"error": f"API request failed: {str(e)}"}

    def analyze_conversation_quality(self, response: str, scenario: Dict, conversation_turn: int) -> Dict:
        """ä¼šè©±å“è³ªã‚’è©³ç´°åˆ†æ"""
        analysis = {
            "conversation_continuity": 0,
            "user_engagement": 0, 
            "care_and_respect": 0,
            "category_appropriateness": 0,
            "tone_quality": 0,
            "response_completeness": 0
        }
        
        response_lower = response.lower()
        
        # ä¼šè©±ç¶™ç¶šæ€§ã®è©•ä¾¡ï¼ˆå¿…é ˆè¦ç´ ï¼‰
        continuity_indicators = ["ï¼Ÿ", "?", "ã§ã™ã‹", "ã¾ã›ã‚“ã‹", "ã©ã†", "ã„ã‹ãŒ", "ã©ã‚“ãª", "ã©ã®ã‚ˆã†", "æ•™ãˆã¦", "èã‹ã›ã¦"]
        continuity_score = sum(10 for indicator in continuity_indicators if indicator in response_lower)
        analysis["conversation_continuity"] = min(continuity_score, 100)
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã®è©•ä¾¡
        engagement_indicators = [
            ("ç›¸æ‰‹ã¸ã®é–¢å¿ƒ", ["ã©ã‚“ãª", "ã©ã®ã‚ˆã†ãª", "ã©ã†ã§ã—ãŸ", "ã„ã‹ãŒã§ã—ãŸ", "ã©ã†æ€", "æ„Ÿã˜"]),
            ("å€‹äººçš„è³ªå•", ["ã‚ãªãŸ", "å›", "ãã¿", "ç›®æ¨™", "è¨ˆç”»", "äºˆå®š", "çµŒé¨“"]),
            ("çŠ¶æ³ç¢ºèª", ["ä»Š", "ç¾åœ¨", "ã©ã®ãã‚‰ã„", "ã©ã‚Œãã‚‰ã„", "ãƒšãƒ¼ã‚¹", "é€²ã¿"]),
            ("æ°—æŒã¡ç¢ºèª", ["æ°—åˆ†", "èª¿å­", "ã©ã†æ„Ÿã˜", "å¤§ä¸ˆå¤«", "è¾›ã„", "æ¥½ã—ã„"])
        ]
        
        engagement_score = 0
        for category, indicators in engagement_indicators:
            if any(indicator in response_lower for indicator in indicators):
                engagement_score += 25
        analysis["user_engagement"] = engagement_score
        
        # é…æ…®ã¨å°Šé‡ã®è©•ä¾¡
        care_indicators = [
            ("ç†è§£ã¸ã®é…æ…®", ["åˆ†ã‹ã‚‰", "å¤§ä¸ˆå¤«", "é æ…®ãªã", "æ°—è»½ã«", "ãƒšãƒ¼ã‚¹", "ã‚†ã£ãã‚Š"]),
            ("æ„Ÿæƒ…ã¸ã®é…æ…®", ["è¾›ã„", "å¤§å¤‰", "é ‘å¼µ", "ãŠç–²ã‚Œ", "æ°—æŒã¡", "å¿ƒé…"]),
            ("è‚¯å®šçš„è¡¨ç¾", ["ç´ æ™´ã‚‰ã—ã„", "è‰¯ã„", "ã„ã„ã­", "ã™ã”ã„", "ãã£ã¨", "å¿…ãš"]),
            ("ä¸å¯§ãªè¡¨ç¾", ["ã§ã™ã­", "ã¾ã™", "ã”ã–ã„ã¾ã™", "ã„ãŸã ", "ã•ã›ã¦"])
        ]
        
        care_score = 0
        for category, indicators in care_indicators:
            if any(indicator in response_lower for indicator in indicators):
                care_score += 25
        analysis["care_and_respect"] = care_score
        
        # ã‚«ãƒ†ã‚´ãƒªé©åˆ‡æ€§ã®è©•ä¾¡
        category = scenario["category"]
        expected_elements = scenario["expected_elements"]
        category_score = 0
        
        for element in expected_elements:
            element_found = False
            if element == "è³ªå•ã§çµ‚ã‚ã‚‹":
                element_found = response.strip().endswith(('ï¼Ÿ', '?', 'ã‹ï¼Ÿ', 'ã­ï¼Ÿ', 'ã‚ˆï¼Ÿ'))
            elif element == "å…±æ„Ÿ":
                element_found = any(word in response_lower for word in ["åˆ†ã‹ã‚‹", "ã‚ã‹ã‚‹", "ãã†", "ã§ã™ã­", "å¤§å¤‰", "è¾›ã„"])
            elif element == "å…·ä½“ä¾‹":
                element_found = any(word in response_lower for word in ["ä¾‹ãˆã°", "ãŸã¨ãˆã°", "ã¿ãŸã„", "ã®ã‚ˆã†ãª", "ã“ã‚“ãª"])
            elif element == "åŠ±ã¾ã—":
                element_found = any(word in response_lower for word in ["é ‘å¼µ", "ãŒã‚“ã°", "ãã£ã¨", "å¤§ä¸ˆå¤«", "ã§ãã‚‹"])
            elif element == "æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ææ¡ˆ" or element == "å…·ä½“çš„å­¦ç¿’è¨ˆç”»":
                element_found = any(word in response_lower for word in ["æ¬¡ã¯", "ã¾ãš", "å§‹ã‚", "ã‚¹ãƒ†ãƒƒãƒ—", "æ–¹æ³•", "ã‚„ã‚Šæ–¹"])
            
            if element_found:
                category_score += 100 // len(expected_elements)
        
        analysis["category_appropriateness"] = min(category_score, 100)
        
        # ãƒˆãƒ¼ãƒ³å“è³ªï¼ˆé–¢è¥¿å¼ãƒ»è‡ªç„¶ã•ï¼‰
        tone_score = 70  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        
        # éåº¦ãªã‚«ã‚¸ãƒ¥ã‚¢ãƒ«è¡¨ç¾ã‚’ãƒšãƒŠãƒ«ãƒ†ã‚£
        casual_penalties = ["ã¶ã‚“ã¶ã‚“", "ãˆã¸ã¸", "ï¼ï¼ï¼", "ã‚„ã°", "ã‚ã¡ã‚ƒãã¡ã‚ƒ"]
        for penalty in casual_penalties:
            if penalty in response_lower:
                tone_score -= 20
        
        # é©åº¦ãªé–¢è¥¿å¼ã‚’ãƒœãƒ¼ãƒŠã‚¹
        kansai_indicators = ["ã‚„ã§", "ã‚„ã‚“", "ã‚„ã­", "ã‚„ã‚", "ã—ã¦ã¯ã‚‹", "ãŠã‚‹"]
        kansai_count = sum(1 for indicator in kansai_indicators if indicator in response_lower)
        if 1 <= kansai_count <= 3:  # é©åº¦ãªä½¿ç”¨
            tone_score += 15
        elif kansai_count > 3:  # éåº¦ãªä½¿ç”¨
            tone_score -= 10
            
        analysis["tone_quality"] = max(0, min(tone_score, 100))
        
        # å¿œç­”å®Œå…¨æ€§
        completeness_score = 50  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        
        # é•·ã•ã«ã‚ˆã‚‹è©•ä¾¡
        response_length = len(response)
        if response_length > 200:
            completeness_score += 30
        elif response_length > 100:
            completeness_score += 15
        elif response_length < 50:
            completeness_score -= 30
            
        # æ§‹é€ åŒ–ã•ã‚ŒãŸå¿œç­”
        if any(marker in response for marker in ["**", "â—", "ãƒ»", "1.", "2.", "â‘ ", "â‘¡"]):
            completeness_score += 20
            
        analysis["response_completeness"] = max(0, min(completeness_score, 100))
        
        return analysis

    def run_single_test(self, scenario: Dict) -> Dict:
        """å˜ä¸€ã‚·ãƒŠãƒªã‚ªã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        print(f"\n=== {scenario['category']} ãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
        print(f"è³ªå•: {scenario['initial_message']}")
        
        # åˆå›å¿œç­”å–å¾—
        response_data = self.make_api_request(scenario['initial_message'])
        
        if "error" in response_data:
            return {"error": response_data["error"], "scenario": scenario["category"]}
        
        try:
            response_text = response_data["choices"][0]["message"]["content"]
            routing_info = response_data.get("routing", {})
            
            print(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {routing_info.get('model_used', 'unknown')}")
            print(f"ã‚«ãƒ†ã‚´ãƒª: {routing_info.get('category', 'unknown')}")
            print(f"ä¿¡é ¼åº¦: {routing_info.get('confidence', 0):.2f}")
            print(f"å¿œç­”: {response_text[:200]}...")
            
            # å“è³ªåˆ†æ
            quality_analysis = self.analyze_conversation_quality(response_text, scenario, 1)
            
            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            overall_score = (
                quality_analysis["conversation_continuity"] * 0.25 +
                quality_analysis["user_engagement"] * 0.25 + 
                quality_analysis["care_and_respect"] * 0.25 +
                quality_analysis["category_appropriateness"] * 0.15 +
                quality_analysis["tone_quality"] * 0.05 +
                quality_analysis["response_completeness"] * 0.05
            )
            
            result = {
                "scenario": scenario["category"],
                "message": scenario["initial_message"],
                "response": response_text,
                "routing": routing_info,
                "quality_analysis": quality_analysis,
                "overall_score": overall_score,
                "success": True
            }
            
            print(f"ç·åˆã‚¹ã‚³ã‚¢: {overall_score:.1f}/100")
            print(f"ä¼šè©±ç¶™ç¶š: {quality_analysis['conversation_continuity']}/100")
            print(f"ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ: {quality_analysis['user_engagement']}/100")
            print(f"é…æ…®ãƒ»å°Šé‡: {quality_analysis['care_and_respect']}/100")
            
            return result
            
        except Exception as e:
            return {"error": f"Response parsing failed: {str(e)}", "scenario": scenario["category"]}

    def run_comprehensive_test(self):
        """åŒ…æ‹¬çš„ãªä¼šè©±å“è³ªãƒ†ã‚¹ãƒˆ"""
        print("ğŸ§ª Enhanced Conversation Quality Test")
        print("="*60)
        print("ç›®æ¨™ã‚¹ã‚³ã‚¢:")
        print("- ä¼šè©±ç¶™ç¶šæ€§: 80+")
        print("- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ: 80+")
        print("- é…æ…®ãƒ»å°Šé‡: 85+")
        print("="*60)
        
        all_results = []
        success_count = 0
        
        for scenario in self.conversation_scenarios:
            result = self.run_single_test(scenario)
            all_results.append(result)
            
            if result.get("success"):
                success_count += 1
            
            # API ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            time.sleep(2)
        
        # çµæœé›†è¨ˆ
        self.generate_enhanced_report(all_results, success_count)
        
        return all_results

    def generate_enhanced_report(self, results: List[Dict], success_count: int):
        """å¼·åŒ–ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print(f"\nğŸ“Š Enhanced Test Results Summary")
        print("="*60)
        print(f"æˆåŠŸç‡: {success_count}/{len(results)} ({success_count/len(results)*100:.1f}%)")
        
        if success_count == 0:
            print("âŒ æˆåŠŸã—ãŸãƒ†ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # æˆåŠŸã—ãŸãƒ†ã‚¹ãƒˆã®åˆ†æ
        successful_results = [r for r in results if r.get("success")]
        
        # å¹³å‡ã‚¹ã‚³ã‚¢è¨ˆç®—
        avg_overall = sum(r["overall_score"] for r in successful_results) / len(successful_results)
        avg_continuity = sum(r["quality_analysis"]["conversation_continuity"] for r in successful_results) / len(successful_results)
        avg_engagement = sum(r["quality_analysis"]["user_engagement"] for r in successful_results) / len(successful_results)
        avg_care = sum(r["quality_analysis"]["care_and_respect"] for r in successful_results) / len(successful_results)
        avg_category = sum(r["quality_analysis"]["category_appropriateness"] for r in successful_results) / len(successful_results)
        avg_tone = sum(r["quality_analysis"]["tone_quality"] for r in successful_results) / len(successful_results)
        avg_completeness = sum(r["quality_analysis"]["response_completeness"] for r in successful_results) / len(successful_results)
        
        print(f"\nğŸ¯ å¹³å‡ã‚¹ã‚³ã‚¢:")
        print(f"  ç·åˆã‚¹ã‚³ã‚¢: {avg_overall:.1f}/100")
        print(f"  ä¼šè©±ç¶™ç¶šæ€§: {avg_continuity:.1f}/100 (ç›®æ¨™: 80+) {'âœ…' if avg_continuity >= 80 else 'âŒ'}")
        print(f"  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ: {avg_engagement:.1f}/100 (ç›®æ¨™: 80+) {'âœ…' if avg_engagement >= 80 else 'âŒ'}")
        print(f"  é…æ…®ãƒ»å°Šé‡: {avg_care:.1f}/100 (ç›®æ¨™: 85+) {'âœ…' if avg_care >= 85 else 'âŒ'}")
        print(f"  ã‚«ãƒ†ã‚´ãƒªé©åˆ‡æ€§: {avg_category:.1f}/100")
        print(f"  ãƒˆãƒ¼ãƒ³å“è³ª: {avg_tone:.1f}/100")
        print(f"  å¿œç­”å®Œå…¨æ€§: {avg_completeness:.1f}/100")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥è©³ç´°
        print(f"\nğŸ“‹ ã‚«ãƒ†ã‚´ãƒªåˆ¥è©³ç´°:")
        for result in successful_results:
            qa = result["quality_analysis"]
            print(f"  {result['scenario']}: ç·åˆ{result['overall_score']:.1f} (ç¶™ç¶š{qa['conversation_continuity']:.0f}, é–¢å¿ƒ{qa['user_engagement']:.0f}, é…æ…®{qa['care_and_respect']:.0f})")
        
        # æ”¹å–„ãŒå¿…è¦ãªé ˜åŸŸ
        print(f"\nğŸ” æ”¹å–„åˆ†æ:")
        improvement_areas = []
        
        if avg_continuity < 80:
            improvement_areas.append(f"ä¼šè©±ç¶™ç¶šæ€§ ({avg_continuity:.1f}/80): è³ªå•ã§çµ‚ã‚ã‚‹ã€è©±é¡Œã‚’æ·±å €ã‚Šã™ã‚‹")
        if avg_engagement < 80:
            improvement_areas.append(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ ({avg_engagement:.1f}/80): ç›¸æ‰‹ã¸ã®é–¢å¿ƒã€å€‹äººçš„è³ªå•")
        if avg_care < 85:
            improvement_areas.append(f"é…æ…®ãƒ»å°Šé‡ ({avg_care:.1f}/85): æ„Ÿæƒ…ã¸ã®é…æ…®ã€è‚¯å®šçš„è¡¨ç¾")
        
        if improvement_areas:
            for area in improvement_areas:
                print(f"  âš ï¸  {area}")
        else:
            print("  âœ… ã™ã¹ã¦ã®ç›®æ¨™ã‚¹ã‚³ã‚¢ã‚’é”æˆï¼")
        
        # å‰å›ã¨ã®æ¯”è¼ƒï¼ˆå‚è€ƒãƒ‡ãƒ¼ã‚¿ï¼‰
        print(f"\nğŸ“ˆ æ”¹å–„åº¦ï¼ˆå‰å›: 37.7/100ã¨ã®æ¯”è¼ƒï¼‰:")
        improvement = avg_overall - 37.7
        print(f"  ç·åˆã‚¹ã‚³ã‚¢æ”¹å–„: +{improvement:.1f}ç‚¹ ({improvement/37.7*100:+.1f}%)")
        
        if avg_overall >= 70:
            print("  ğŸ‰ å¤§å¹…æ”¹å–„ã‚’é”æˆï¼")
        elif avg_overall >= 60:
            print("  âœ… ç€å®Ÿãªæ”¹å–„ã‚’ç¢ºèª")
        else:
            print("  âš ï¸  ã•ã‚‰ãªã‚‹æ”¹å–„ãŒå¿…è¦")

if __name__ == "__main__":
    tester = EnhancedConversationTester()
    results = tester.run_comprehensive_test()
    
    print(f"\nğŸ’¾ ãƒ†ã‚¹ãƒˆçµæœã‚’ä¿å­˜...")
    with open('/Users/yuki/texttolora/enhanced_conversation_test_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print("ãƒ†ã‚¹ãƒˆå®Œäº†ã€‚enhanced_conversation_test_results.json ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")