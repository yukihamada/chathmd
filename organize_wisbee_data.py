#!/usr/bin/env python3
"""
Wisbeeãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿æ•´ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ—¢å­˜ã®Wisbeeãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã€ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡ã—ã¦
100ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã™ã€‚
"""

import json
import os
import re
from collections import defaultdict
import hashlib

def load_jsonl_file(file_path):
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    if not os.path.exists(file_path):
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        return []
    
    data = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        item = json.loads(line)
                        data.append(item)
                    except json.JSONDecodeError as e:
                        print(f"JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ {file_path}:{line_num}: {e}")
    except Exception as e:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
    
    return data

def categorize_sample(sample):
    """ã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡"""
    text = ""
    if 'conversations' in sample:
        for conv in sample['conversations']:
            text += conv.get('value', '') + " "
    elif 'instruction' in sample and 'output' in sample:
        text += sample['instruction'] + " " + sample['output']
    elif 'messages' in sample:
        for msg in sample['messages']:
            text += msg.get('content', '') + " "
    
    text = text.lower()
    
    # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°é–¢é€£
    programming_keywords = [
        'python', 'javascript', 'html', 'css', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'ã‚³ãƒ¼ãƒ‰', 'ãƒã‚°', 
        'ãƒ‡ãƒãƒƒã‚°', 'ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ', 'ãƒ‡ãƒ¼ã‚¿æ§‹é€ ', 'é–¢æ•°', 'ã‚¯ãƒ©ã‚¹', 'ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ',
        'api', 'ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯', 'ãƒ©ã‚¤ãƒ–ãƒ©ãƒª', 'git', 'github', 'sql', 'database'
    ]
    
    # ç§‘å­¦ãƒ»æ•°å­¦é–¢é€£
    science_keywords = [
        'æ•°å­¦', 'ç‰©ç†', 'åŒ–å­¦', 'ç”Ÿç‰©', 'ç§‘å­¦', 'å®Ÿé¨“', 'ç†è«–', 'å…¬å¼', 'æ–¹ç¨‹å¼',
        'çµ±è¨ˆ', 'ç¢ºç‡', 'å¾®ç©åˆ†', 'ä»£æ•°', 'å¹¾ä½•', 'é‡å­', 'ç›¸å¯¾æ€§ç†è«–', 'dna'
    ]
    
    # ã‚¢ãƒ¼ãƒˆãƒ»æ–‡åŒ–é–¢é€£
    art_keywords = [
        'ã‚¢ãƒ¼ãƒˆ', 'èŠ¸è¡“', 'ç¾è¡“', 'éŸ³æ¥½', 'æ–‡å­¦', 'è©©', 'å°èª¬', 'æ˜ ç”»', 'æ¼”åŠ‡',
        'çµµç”»', 'å½«åˆ»', 'ãƒ‡ã‚¶ã‚¤ãƒ³', 'æ–‡åŒ–', 'æ­´å²', 'å“²å­¦', 'å®—æ•™', 'ä¼çµ±'
    ]
    
    # Hamadaé–¢é€£ï¼ˆç‰¹å®šã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ï¼‰
    hamada_keywords = ['hamada', 'ãƒãƒãƒ€', 'æµœç”°']
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
    if any(keyword in text for keyword in hamada_keywords):
        return 'hamada'
    elif any(keyword in text for keyword in programming_keywords):
        return 'programming'
    elif any(keyword in text for keyword in science_keywords):
        return 'science_math'
    elif any(keyword in text for keyword in art_keywords):
        return 'art_culture'
    else:
        return 'general'

def create_chunks(data, chunk_size=100):
    """ãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡å®šã‚µã‚¤ã‚ºã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
    chunks = []
    for i in range(0, len(data), chunk_size):
        chunks.append(data[i:i + chunk_size])
    return chunks

def save_chunks(category, chunks, base_dir):
    """ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    category_dir = os.path.join(base_dir, category)
    os.makedirs(category_dir, exist_ok=True)
    
    for i, chunk in enumerate(chunks, 1):
        filename = f"{category}_chunk_{i:03d}.jsonl"
        filepath = os.path.join(category_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            for item in chunk:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        print(f"ä¿å­˜å®Œäº†: {filepath} ({len(chunk)}ã‚µãƒ³ãƒ—ãƒ«)")

def remove_duplicates(data):
    """é‡è¤‡ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚’é™¤å»"""
    seen_hashes = set()
    unique_data = []
    
    for item in data:
        # ã‚µãƒ³ãƒ—ãƒ«ã®å†…å®¹ã‚’ãƒãƒƒã‚·ãƒ¥åŒ–
        content = json.dumps(item, sort_keys=True, ensure_ascii=False)
        content_hash = hashlib.md5(content.encode()).hexdigest()
        
        if content_hash not in seen_hashes:
            seen_hashes.add(content_hash)
            unique_data.append(item)
    
    return unique_data

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ Wisbeeãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿æ•´ç†é–‹å§‹")
    
    # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
    input_files = [
        'wisbee_final_training_data.jsonl',
        'wisbee_complete_training_data.jsonl',
        'wisbee_training_data.jsonl',
        'wisbee_extended_training_data.jsonl',
        'balanced_wisbee_training_data.jsonl',
        'wisbee_hamada_training_data.jsonl',
        'wisbee_model_nft_training_data.jsonl'
    ]
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
    all_data = []
    file_stats = {}
    
    for file_path in input_files:
        print(f"\nğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­: {file_path}")
        data = load_jsonl_file(file_path)
        file_stats[file_path] = len(data)
        all_data.extend(data)
        print(f"   èª­ã¿è¾¼ã¿å®Œäº†: {len(data)}ã‚µãƒ³ãƒ—ãƒ«")
    
    print(f"\nğŸ“Š ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(all_data)}ã‚µãƒ³ãƒ—ãƒ«")
    
    # é‡è¤‡é™¤å»
    print("\nğŸ”„ é‡è¤‡é™¤å»ä¸­...")
    unique_data = remove_duplicates(all_data)
    print(f"   é‡è¤‡é™¤å»å¾Œ: {len(unique_data)}ã‚µãƒ³ãƒ—ãƒ«")
    print(f"   é™¤å»ã•ã‚ŒãŸé‡è¤‡: {len(all_data) - len(unique_data)}ã‚µãƒ³ãƒ—ãƒ«")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†é¡
    print("\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†é¡ä¸­...")
    categorized_data = defaultdict(list)
    
    for sample in unique_data:
        category = categorize_sample(sample)
        categorized_data[category].append(sample)
    
    # åˆ†é¡çµæœã®è¡¨ç¤º
    print("\nğŸ“ˆ åˆ†é¡çµæœ:")
    total_samples = 0
    for category, data in categorized_data.items():
        print(f"   {category}: {len(data)}ã‚µãƒ³ãƒ—ãƒ«")
        total_samples += len(data)
    
    print(f"   åˆè¨ˆ: {total_samples}ã‚µãƒ³ãƒ—ãƒ«")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™
    output_dir = "organized_wisbee_data"
    os.makedirs(output_dir, exist_ok=True)
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã¦ä¿å­˜
    print(f"\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­ï¼ˆ{output_dir}ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰...")
    chunk_info = {}
    
    for category, data in categorized_data.items():
        print(f"\nğŸ“ {category}ã‚«ãƒ†ã‚´ãƒªå‡¦ç†ä¸­...")
        chunks = create_chunks(data, chunk_size=100)
        save_chunks(category, chunks, output_dir)
        chunk_info[category] = {
            'total_samples': len(data),
            'total_chunks': len(chunks),
            'samples_per_chunk': 100
        }
    
    # çµ±è¨ˆæƒ…å ±ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    stats = {
        'input_files': file_stats,
        'total_original_samples': len(all_data),
        'total_unique_samples': len(unique_data),
        'duplicates_removed': len(all_data) - len(unique_data),
        'categories': chunk_info,
        'output_directory': output_dir
    }
    
    stats_file = os.path.join(output_dir, 'organization_stats.json')
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“Š çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜: {stats_file}")
    print("\nâœ… ãƒ‡ãƒ¼ã‚¿æ•´ç†å®Œäº†ï¼")
    
    # ã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º
    print("\n" + "="*50)
    print("ğŸ“‹ æ•´ç†ã‚µãƒãƒªãƒ¼")
    print("="*50)
    print(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(input_files)}")
    print(f"å…ƒãƒ‡ãƒ¼ã‚¿ç·æ•°: {len(all_data):,}ã‚µãƒ³ãƒ—ãƒ«")
    print(f"é‡è¤‡é™¤å»å¾Œ: {len(unique_data):,}ã‚µãƒ³ãƒ—ãƒ«")
    print(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    print("\nã‚«ãƒ†ã‚´ãƒªåˆ¥å†…è¨³:")
    for category, info in chunk_info.items():
        print(f"  {category}: {info['total_samples']:,}ã‚µãƒ³ãƒ—ãƒ« ({info['total_chunks']}ãƒãƒ£ãƒ³ã‚¯)")

if __name__ == "__main__":
    main()